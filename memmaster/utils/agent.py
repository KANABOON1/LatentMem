from dataclasses import dataclass, fields
import os
from string import Formatter
from typing import Optional

import torch
from transformers import PreTrainedModel, PreTrainedTokenizer, GenerationConfig

from common.utils.tensor_utils import erase_after_first_eos, pad_batch_embeds
from memmaster.mas_core.base_centralized_memory import BaseCentralizedMemory, Memory
from memmaster.utils.message import MessageNode

# NOTE - 统计 MAS 中消耗的 tokens(不考虑 memory 部分)
TOKEN_CONSUMPTIONS = 0

EMPTY_MEMORY = "<|Memory_Empty|>"  # text memory 为 None 时的 prompt 占位符
MAX_LEN = 1024  # 在 agent.forward 时限制的最大长度(如果超出了长度则左裁剪)

@dataclass
class LLMAgent:
    # --- Agent settings ---
    # Model for this Agent
    model: Optional[PreTrainedModel] = None
    tokenizer: Optional[PreTrainedTokenizer] = None
    
    # Agent name
    role: Optional[str] = None
    # Agent UUID (autogenerated if not set)
    id: Optional[str] = None
    # Agent communication topology id in MAS
    topology_node_id: Optional[int] = None

    # --- System message settings ---
    system_prompt_template: Optional[str] = None
    # Role for the system message
    system_prompt_role: str = "system"

    # --- User message settings ---
    user_prompt_template: Optional[str] = None
    # Role for the user message
    user_prompt_role: str = "user"   

    # --- Agent Memory ---
    # MAS 中 agent 的中心记忆池
    centralized_memory: Optional[BaseCentralizedMemory] = None
    
    def forward(
        self,
        system_prompt_templates: list[str],
        system_prompt_fields: list[dict],
        user_prompt_templates: list[str],
        user_prompt_fields: list[dict],
        responses: list[str],
        states: list[dict],
    ) -> dict:
        # 替换 state 之前保存 agent 原来的 state
        origin_state = self._state

        # 循环, 对于每一个 sample, 拼接: prompt + latent tokens + reponse embeddings
        batch_inputs_embeds = []
        batch_labels = []
        batch_attention_masks = []
        completion_lengths = []
        for i in range(len(system_prompt_templates)):

            # 构造 system, user prompt 和 response
            system_template = system_prompt_templates[i]
            system_field = system_prompt_fields[i]
            user_template = user_prompt_templates[i]
            user_field = user_prompt_fields[i]
            response = responses[i]
            state = states[i]
            
            assert state.get("system_prompt_template") == system_template
            assert state.get("user_prompt_template") == user_template
            
            # 还原先前 agent 信息
            self._reconstruct_state(state)
            
            system_prompt = self._safe_format(system_template, system_field)

            # 将 memory content 去除: 希望如果存在 latent memory 就不要使用 text memory
            text_memory = user_field.pop("memory_content")  
            user_field["memory_content"] = EMPTY_MEMORY
            user_prompt = self._safe_format(user_template, user_field)
            
            # 根据 text memory 恢复为完整的 memory
            task_description = user_field.get("task_description")
            extra_fields = user_field.get("extra_fields")
            memory = self._recover_memory(text_memory, task_description, extra_fields)
            
            # 构造 prompt embeds
            full_prompt_tokens = self.tokenizer.apply_chat_template(
                [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                add_generation_prompt=True,  
                enable_thinking=False,
                return_tensors="pt"
            ).to(self.model.device)
            
            model_embedding_layer = self.model.get_input_embeddings()
            with torch.no_grad():
                text_embeddings = model_embedding_layer(full_prompt_tokens)
            
            latent_emb = memory.latent_memory.to(self.model.device) if memory.latent_memory is not None else None
            if latent_emb is not None:
                if latent_emb.dim() == 2:
                    latent_emb = latent_emb.unsqueeze(0)
                prompt_embeds = torch.cat([text_embeddings, latent_emb], dim=1)  # TODO - 暂时把 latent memory 拼在后面
            else:
                prompt_embeds = text_embeddings
            
            # 直接 encode reponse str, 并将其拼在 prompt_embeds 之后
            response_tokenizer_output = self.tokenizer(
                response,  
                return_tensors="pt",
                add_special_tokens=False 
            ) 
            response_tokens = response_tokenizer_output.input_ids.to(self.model.device)
            response_tokens = response_tokens.to(self.model.device).long()
            
            # 将 eos token id 追加到 response_tokens 后
            eos_token_id = self.tokenizer.eos_token_id
            batch_size = response_tokens.shape[0]
            eos_token = torch.tensor([[eos_token_id]] * batch_size, dtype=torch.long, device=self.model.device)
            response_tokens = torch.cat([response_tokens, eos_token], dim=-1)
            response_embeds = model_embedding_layer(response_tokens)

            full_inputs_embeds = torch.cat([prompt_embeds, response_embeds], dim=1)
            batch_inputs_embeds.append(full_inputs_embeds)
            prompt_len = prompt_embeds.size(1)
            
            # 构造 labels
            prompt_labels = torch.full(
                (1, prompt_len), 
                -100, 
                dtype=torch.long, 
                device=self.model.device
            )
            response_labels = response_tokens.clone().detach() 
            full_labels = torch.cat([prompt_labels, response_labels], dim=1)
            batch_labels.append(full_labels)
            
            # 构造 attention mask
            prompt_mask = torch.ones((1, prompt_len), dtype=torch.long, device=self.model.device)
            response_mask = torch.ones((1, response_tokens.shape[1]), dtype=torch.long, device=self.model.device)
            full_attention_mask = torch.cat([prompt_mask, response_mask], dim=1)
            batch_attention_masks.append(full_attention_mask)
            completion_lengths.append(response_embeds.size(1))

            assert full_labels.shape == full_attention_mask.shape == full_inputs_embeds.shape[:2]

        # pad batch: right padding 
        final_inputs_embeds, final_labels, final_attention_mask = pad_batch_embeds(
            batch_inputs_embeds, 
            batch_labels, 
            batch_attention_masks,
            model_embedding_layer,
            self.tokenizer.pad_token_id,
            self.model.device
        )

        seq_len = final_inputs_embeds.size(1)
        origin_labels = final_labels  

        if seq_len > MAX_LEN:
            start_index = seq_len - MAX_LEN
            final_inputs_embeds = final_inputs_embeds[:, start_index:, :]
            final_labels = final_labels[:, start_index:] 
            final_attention_mask = final_attention_mask[:, start_index:]
        
        # model forward
        model_dtype = next(self.model.parameters()).dtype
        final_inputs_embeds = final_inputs_embeds.to(model_dtype)
        
        outputs = self.model.forward(
            inputs_embeds=final_inputs_embeds,
            attention_mask=final_attention_mask,
            labels=final_labels,
            output_hidden_states=False,
            output_attentions=False,
            use_cache=False,       
            return_dict=True       
        )

        logits = outputs.logits
        B, truncated_len, D = logits.shape   # batch, truncated_len, vocab_size
        
        # 确保 outputs 返回的 logits 与输入的形状一致
        if seq_len > MAX_LEN:
            padded_logits = torch.zeros(B, seq_len, D, dtype=logits.dtype, device=logits.device)
            start_index = seq_len - MAX_LEN
            padded_logits[:, start_index:, :] = logits 
            outputs.logits = padded_logits
        else:
            outputs.logits = logits
        
        # outputs 额外返回 labels
        if seq_len > MAX_LEN:
            padded_labels = origin_labels.clone()
            start_index = seq_len - MAX_LEN
            padded_labels[:, :start_index + 1] = -100  # NOTE - 如果  
            outputs["labels"] = padded_labels
        else:
            outputs["labels"] = origin_labels
        
        # 检查 logits 和 labels 是否对齐
        self._check_logits_and_labels(outputs.logits, outputs["labels"])

        # 更改回原来的 state
        self._reconstruct_state(origin_state)
        
        return outputs
    
    @torch.no_grad()
    def invoke(
        self, 
        system_batch_inputs: dict[str, list[str]], 
        user_batch_inputs: dict[str, list[str]], 
        generation_config: GenerationConfig,
        **kwargs
    ) -> list[MessageNode]:
        """激活 centralized memory, 生成关于这个问题的回答"""
        
        batch_inputs_embeds = []
        batch_input_ids = []
        batch_size = len(next(iter(system_batch_inputs.values())))
        
        # --- 逐样本收集 memory, 构造输入 prompt ---
        batch_system_fields: list[dict[str]] = []
        batch_user_fields: list[dict[str]] = []
        for i in range(batch_size):
            # 解析 sample
            system_sample = {k: v[i] for k, v in system_batch_inputs.items()}
            user_sample = {k: v[i] for k, v in user_batch_inputs.items()}
            # self._check_inputs(system_sample, user_sample)
            
            # 激活 centralized memory: 根据当前的 prompt (task_desc) 从 memory 中 query 出信息
            task_description = user_sample.get("task_description")
            experience = self._trigger_memory(task_description)
            user_sample["memory_content"] = experience.text_memory
            # NOTE - extra_fields 中的 `role_message` 字段保存了该 agent 在 mas 中的所有 response 
            # 在 invoke 阶段 extra_fields 中的内容不会被使用到, 但是在 SFT 时需要根据 extra_fields 中的字段重建 latent_memory
            # 因此在 invoke 阶段需要将 extra_fields 中的内容保存到 database 中, 用于构造 SFT 时所需的数据
            user_sample["extra_fields"] = experience.extra_fields  

            # 构造输入给 LLM 的文本 Prompt
            system_str, user_str = self._format_llm_input_prompt(
                system_fields=system_sample,
                user_fields=user_sample
            )

            # 构造完整 prompts
            full_prompt_tokens = self.tokenizer.apply_chat_template(
                [{"role": "system", "content": system_str},
                {"role": "user", "content": user_str}],
                add_generation_prompt=True,
                enable_thinking=False,
                return_tensors="pt"
            ) 
            self._add_token_consumptions(full_prompt_tokens)

            model_embedding_layer = self.model.get_input_embeddings()
            text_embeddings = model_embedding_layer(full_prompt_tokens.to(self.model.device))
            
            # Latent Embedding 拼接: 将提取出来的记忆直接拼接到所有的文本输入之后
            latent_emb = experience.latent_memory.to(self.model.device) if experience.latent_memory is not None else None
             
            self._save_latents_to_local(latent_emb)  # NOTE - 将该 agent 得到的 latent meomry 保存到本地, 验证 LatentMem 是否可以定制化记忆

            if latent_emb is not None:
                if latent_emb.dim() == 2:
                    latent_emb = latent_emb.unsqueeze(0)
                
                final_inputs_embeds = torch.cat([text_embeddings, latent_emb], dim=1)
                batch_inputs_embeds.append(final_inputs_embeds.squeeze(0)) 
                batch_input_ids.append(None)
            else:
                batch_inputs_embeds.append(text_embeddings.squeeze(0))
                batch_input_ids.append(full_prompt_tokens.squeeze(0))
            
            # 记录该 sample 的 system 和 user fields
            batch_system_fields.append(system_sample)
            batch_user_fields.append(user_sample)
        
        # --- preprocess inputs (add pad tokens) ---
        # pad batch embeddings
        max_seq_len = max([emb.shape[0] for emb in batch_inputs_embeds])
        padded_inputs_embeds = []
        for emb in batch_inputs_embeds:
            padding_needed = max_seq_len - emb.shape[0]
            if padding_needed > 0:
                
                padding_tensor = torch.zeros(
                    (padding_needed, emb.shape[1]), 
                    dtype=emb.dtype, 
                    device=self.model.device
                )

                padded_emb = torch.cat([padding_tensor, emb], dim=0)
            else:
                padded_emb = emb
            padded_inputs_embeds.append(padded_emb) 
            
        final_batch_inputs_embeds = torch.stack(padded_inputs_embeds, dim=0)

        # 创建 attention mask
        attention_mask = torch.zeros(
            (final_batch_inputs_embeds.shape[0], final_batch_inputs_embeds.shape[1]), 
            device=self.model.device
        )
        for i, emb in enumerate(batch_inputs_embeds):

            original_length = emb.shape[0]
            padding_length = max_seq_len - original_length
            attention_mask[i, padding_length:max_seq_len] = 1
        
        # --- llm generate ---        
        output_ids = self.model.generate(
            inputs_embeds=final_batch_inputs_embeds, 
            attention_mask=attention_mask,
            # **generation_config
            generation_config=generation_config
        )

        # --- postprocess ---
        # 清空多余字符
        output_ids = erase_after_first_eos(output_ids, self.tokenizer.pad_token_id, self.tokenizer.eos_token_id)
        output_strs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        # 构造 message nodes
        message_nodes = []

        # 遍历 batch 中的每个样本构造 message node
        for system_fields, user_fields, output_str in zip(batch_system_fields, batch_user_fields, output_strs):
            node = MessageNode(
                system_prompt_template=self.system_prompt_template,
                system_prompt_fields=system_fields,
                user_prompt_template=self.user_prompt_template,
                user_prompt_fields=user_fields,
                response=output_str,
                state=self._state
            )
            message_nodes.append(node)       

        return message_nodes
    
    @property
    def _state(self) -> dict:
        state = {}

        for field_name, value in self.__dict__.items():
            if field_name in ["model", "tokenizer", "centralized_memory"]:
                if value is None:
                    state[field_name] = None
                else:
                    state[field_name] = str(type(value).__name__)
            else:
                state[field_name] = value

        return state
    
    def _reconstruct_state(self, state: dict):
        for f in fields(self):
            if f.name not in ["model", "tokenizer", "centralized_memory"]:
                setattr(self, f.name, state.get(f.name))

    def _trigger_memory(self, task_description: str) -> Memory:
        """agent 激发 memory 获取 text memory 和 latent memory, 并且进行后处理:
        - latent memory 不为 None: 则保持 latent memory 与 agent.model 的类型一致, 并且将 text memory 置为 Empty 不再使用
        - text memory 为 None: 默认置为 Empty
        """
        if self.centralized_memory is not None:
            memory = self.centralized_memory.retrieve_memory(task_description, self)
            
            # 如果 lat memory 存在, 则 agent 中不使用 text memory
            if memory.latent_memory is not None:
                memory.text_memory = EMPTY_MEMORY
                # memory(float32) -> agent(bfloat16)
                memory.latent_memory = memory.latent_memory.to(self.model.dtype)
            
            # 如果 text memory 不存在, 则对应使用 EMPTY_MEMORY 占位符
            if memory.text_memory is None:
                memory.text_memory = EMPTY_MEMORY
        else:
            memory = Memory(text_memory=EMPTY_MEMORY, latent_memory=None)
        
        return memory
    
    def _recover_memory(self, text_memory: str, task_description: str, extra_fields: dict) -> Memory:
        # 根据 task description 和 extra_fields 得到压缩之后的 latent memory
        if self.centralized_memory is not None:
            assert text_memory is not None
            memory = self.centralized_memory.process_memory(text_memory, task_description, extra_fields, self)
        else:
            assert text_memory is None  # 如果 memory_content 为 None, 则对应的, 现在的 agent 也不应该右 centralized memory
            memory = Memory(text_memory=EMPTY_MEMORY, latent_memory=None)
        
        # 将 latent memory 移动到 agent.model 的 dtype
        if memory.latent_memory is not None:
            memory.latent_memory = memory.latent_memory.to(self.model.dtype)
        
        return memory

    def _safe_format(self, template: str, fields: dict) -> str:
        formatter = Formatter()
        field_names = {fname for _, fname, _, _ in formatter.parse(template) if fname}

        # 检查模板中出现的字段是否都在 fields 中
        missing_fields = field_names - fields.keys()
        if missing_fields:
            raise KeyError(f"Missing fields for formatting: {missing_fields}")

        # 安全过滤
        safe_fields = {k: v for k, v in fields.items() if k in field_names}
        return template.format(**safe_fields)
    
    def _format_llm_input_prompt(self, system_fields: dict, user_fields: dict) -> tuple[str, str]:
        """
        Safely format system and user prompts.
        Any keys in fields that do not appear in the template are ignored.
        """
        system_prompt = self._safe_format(self.system_prompt_template, system_fields)
        user_prompt = self._safe_format(self.user_prompt_template, user_fields)

        return system_prompt, user_prompt

    def _check_logits_and_labels(self, logits: torch.FloatTensor, labels: torch.LongTensor):
        """
        检查 logits 和 labels 在 Shift 后的有效位置是否对应非零 logits 向量。
        该函数通常用于检查语言模型中的 next-token prediction 任务。
        
        Args:
            logits: 模型原始输出，维度通常是 (batch_size, sequence_length, vocab_size)。
            labels: 对应的标签，维度通常是 (batch_size, sequence_length)。

        Raises:
            AssertionError: 如果发现有需要计算损失的位置，其对应的 logits 向量为全 0。
        """
        
        # 1. 对 logits 和 labels 进行 Shift 操作
        # logits 需要预测下一个 token，所以丢弃最后一个时间步 (S-1)
        shift_logits = logits[:, :-1]
        # labels 是正确的下一个 token，所以丢弃第一个时间步 (S+1)
        shift_labels = labels[:, 1:]

        # 2. 检查 shift labels[b][s] 不等于 -100 时, 对应的 shift logits[b][s] 一定不为 0 向量
        
        # --- 步骤 A: 找出需要计算损失的位置 ---
        # is_valid_position 的维度是 (B, S-1)
        is_valid_position = (shift_labels != -100)
        
        # --- 步骤 B: 找出哪些位置的 shift_logits 是全 0 向量 ---
        # 沿着词汇表维度 (dim=-1) 对 shift_logits 的绝对值求和。
        # 如果结果为 0，则表示该位置的 logits 向量是全 0。
        # is_zero_logits_vector 的维度是 (B, S-1)
        # 使用 torch.sum(torch.abs(...)) 来判断是否全为 0，比判断是否全为 0.0 更健壮
        is_zero_logits_vector = (torch.sum(torch.abs(shift_logits), dim=-1) == 0)

        # --- 步骤 C: 找出冲突的位置 ---
        # 冲突条件：位置有效 (True) **并且** logits 为全 0 (True)
        # 即：is_valid_position AND is_zero_logits_vector
        conflict_positions = (is_valid_position & is_zero_logits_vector)

        # --- 步骤 D: 断言所有冲突位置的数量为 0 ---
        if conflict_positions.any():
            num_conflicts = conflict_positions.sum().item()
            # 抛出 AssertionError 是一种在开发和调试中强制检查数据完整性的标准做法
            raise AssertionError(
                f"❌ 数据不一致错误！发现 {num_conflicts} 个需要计算损失的位置 (label != -100),"
                f"但其对应的 shift_logits 向量为全 0。"
            )
    
    # --- analysis ---
    def _save_latents_to_local(self, latents: torch.FloatTensor):
        # 实验分析: 将不同的 agents 对应的 latents 保存到本地, 用于验证 LatentMem 是否能定制化 memory
        # 需要外部设定 self.working_dir 

        if hasattr(self, 'working_dir') and self.working_dir is not None and latents is not None:
            save_dir = os.path.join(self.working_dir, "latents_analysis", self.role)
            
            os.makedirs(save_dir, exist_ok=True)
            
            # 文件编号为 save_dir 下所有 pt 文件的数量
            existing_files = [f for f in os.listdir(save_dir) if f.endswith('.pt')]
            file_idx = len(existing_files)
            file_name = f"{str(file_idx).zfill(4)}.pt"
            file_path = os.path.join(save_dir, file_name)
            
            torch.save(latents.detach().cpu(), file_path)

        else:
            pass
    
    def _add_token_consumptions(self, prompt_tokens: torch.Tensor):
        tokens_count = prompt_tokens.numel()
        
        global TOKEN_CONSUMPTIONS
        TOKEN_CONSUMPTIONS += tokens_count