model:
  
  load_model_path: null

  mas:
    structure: autogen
    llm_name_or_path: Qwen/Qwen3-4B-Instruct-2507
  memory:
    # centralized mmeory name
    name: latentmem
    llm_name_or_path: Qwen/Qwen3-4B-Instruct-2507
    use_weaver: True  

    # rag part
    rag:
      mode: metagpt
      pos_shots_num: 1  
      neg_shots_num: 0  
      insights_num: 3  
      embedding_model_name_or_path: sentence-transformers/all-MiniLM-L6-v2 
      database_dir: null

    # weaver part
    weaver:
      
      latents_len: 8
      # llm peft
      use_peft: True
      peft_config:
        r: 16
        lora_alpha: 32
        target_modules: ["q_proj", "v_proj"]
        lora_dropout: 0.1
        bias: "none"
        task_type: "CAUSAL_LM"


# dataset configs
dataset: 
    name: pddl
    mode: sft
    sft:
      valid_ratio: 0.1
    grpo:
      valid_ratio: 0.1

run: 

  seed: 42
  use_wandb: False
  working_dir: null  
  device: 0  
  
  # route
  mode: data  # mode: data, sft, lmpo, eval

  # sft configs
  sft:
    num_train_epochs: 2
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 1
    
    # optimizer configs
    optim: adamw_torch
    lr_scheduler_type: cosine
    warmup_ratio: 0.1
    learning_rate: 1e-5
    
    # logging
    logging_strategy: steps
    logging_steps: 1
    eval_strategy: epoch
    eval_steps: 100
    save_strategy: epoch
    save_steps: 100

    assistant_only_loss: False   # used only in conversational dataset
    max_length: 2048  

    load_best_model_at_end: True
    remove_unused_columns: False
    bf16: True
    
  # lmpo configs
  lmpo:
    num_train_epochs: 1
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    num_generations: 4   
    num_iterations: 1
    gradient_accumulation_steps: 1
    beta: 0.0
    loss_type: bnpo
    
    max_prompt_length: 1024
    max_completion_length: 256 
    temperature: 1.0
    
    # optimizer configs
    optim: adamw_torch
    lr_scheduler_type: cosine
    warmup_ratio: 0.1
    learning_rate: 1e-5
    
    # duration
    logging_strategy: steps
    logging_steps: 1
    eval_strategy: epoch
    eval_steps: 100
    save_strategy: epoch
    save_steps: 100
    
    load_best_model_at_end: True
    remove_unused_columns: False
    bf16: True
    
  # generation config for llm
  generation:
    max_new_tokens: 512 
    do_sample: False
    temperature: 0.0
    early_stopping: True
    use_cache: True
  
  # interactin config for agent
  interaction:
    batch_size: 4  
    max_turns: 30  
    max_obs_length: 2048  