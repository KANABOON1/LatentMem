from dataclasses import dataclass, fields
from string import Formatter
from typing import Optional

import torch
from transformers import PreTrainedModel, PreTrainedTokenizer, GenerationConfig

from common.utils.tensor_utils import erase_after_first_eos, pad_batch_embeds
from latentmem.mas_core.base_centralized_memory import BaseCentralizedMemory, Memory
from latentmem.utils.message import MessageNode

EMPTY_MEMORY = "<|Memory_Empty|>"
MAX_LEN = 1024  

@dataclass
class LLMAgent:
    # --- Agent settings ---
    # Model for this Agent
    model: Optional[PreTrainedModel] = None
    tokenizer: Optional[PreTrainedTokenizer] = None
    
    # Agent name
    role: Optional[str] = None
    # Agent UUID (autogenerated if not set)
    id: Optional[str] = None
    # Agent communication topology id in MAS
    topology_node_id: Optional[int] = None

    # --- System message settings ---
    system_prompt_template: Optional[str] = None
    # Role for the system message
    system_prompt_role: str = "system"

    # --- User message settings ---
    user_prompt_template: Optional[str] = None
    # Role for the user message
    user_prompt_role: str = "user"   

    # --- Agent Memory ---
    centralized_memory: Optional[BaseCentralizedMemory] = None
    
    def forward(
        self,
        system_prompt_templates: list[str],
        system_prompt_fields: list[dict],
        user_prompt_templates: list[str],
        user_prompt_fields: list[dict],
        responses: list[str],
        states: list[dict],
    ) -> dict:

        origin_state = self._state

        # loop
        batch_inputs_embeds = []
        batch_labels = []
        batch_attention_masks = []
        completion_lengths = []
        for i in range(len(system_prompt_templates)):

            # construct system, user prompt and response
            system_template = system_prompt_templates[i]
            system_field = system_prompt_fields[i]
            user_template = user_prompt_templates[i]
            user_field = user_prompt_fields[i]
            response = responses[i]
            state = states[i]
            
            assert state.get("system_prompt_template") == system_template
            assert state.get("user_prompt_template") == user_template
            
            self._reconstruct_state(state)
            
            system_prompt = self._safe_format(system_template, system_field)

            # remove memory content: if latent memory exists, prefer not to use text memory
            text_memory = user_field.pop("memory_content")  
            user_field["memory_content"] = EMPTY_MEMORY
            user_prompt = self._safe_format(user_template, user_field)
            
            # restore the complete memory from the text memory
            task_description = user_field.get("task_description")
            extra_fields = user_field.get("extra_fields")
            memory = self._recover_memory(text_memory, task_description, extra_fields)
            
            # construct inputs embeds
            full_prompt_tokens = self.tokenizer.apply_chat_template(
                [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
                add_generation_prompt=True,  
                enable_thinking=False,
                return_tensors="pt"
            ).to(self.model.device)
            
            model_embedding_layer = self.model.get_input_embeddings()
            with torch.no_grad():
                text_embeddings = model_embedding_layer(full_prompt_tokens)
            
            latent_emb = memory.latent_memory.to(self.model.device) if memory.latent_memory is not None else None
            if latent_emb is not None:
                if latent_emb.dim() == 2:
                    latent_emb = latent_emb.unsqueeze(0)
                prompt_embeds = torch.cat([text_embeddings, latent_emb], dim=1)
            else:
                prompt_embeds = text_embeddings
            
            response_tokenizer_output = self.tokenizer(
                response,  
                return_tensors="pt",
                add_special_tokens=False 
            ) 
            response_tokens = response_tokenizer_output.input_ids.to(self.model.device)
            response_tokens = response_tokens.to(self.model.device).long()
            
            # append EOS token after response tokens
            eos_token_id = self.tokenizer.eos_token_id
            batch_size = response_tokens.shape[0]
            eos_token = torch.tensor([[eos_token_id]] * batch_size, dtype=torch.long, device=self.model.device)
            response_tokens = torch.cat([response_tokens, eos_token], dim=-1)
            response_embeds = model_embedding_layer(response_tokens)

            full_inputs_embeds = torch.cat([prompt_embeds, response_embeds], dim=1)
            batch_inputs_embeds.append(full_inputs_embeds)
            prompt_len = prompt_embeds.size(1)
            
            # construct labels
            prompt_labels = torch.full(
                (1, prompt_len), 
                -100, 
                dtype=torch.long, 
                device=self.model.device
            )
            response_labels = response_tokens.clone().detach() 
            full_labels = torch.cat([prompt_labels, response_labels], dim=1)
            batch_labels.append(full_labels)
            
            # construct attention mask
            prompt_mask = torch.ones((1, prompt_len), dtype=torch.long, device=self.model.device)
            response_mask = torch.ones((1, response_tokens.shape[1]), dtype=torch.long, device=self.model.device)
            full_attention_mask = torch.cat([prompt_mask, response_mask], dim=1)
            batch_attention_masks.append(full_attention_mask)
            completion_lengths.append(response_embeds.size(1))

            assert full_labels.shape == full_attention_mask.shape == full_inputs_embeds.shape[:2]

        # pad batch: right padding 
        final_inputs_embeds, final_labels, final_attention_mask = pad_batch_embeds(
            batch_inputs_embeds, 
            batch_labels, 
            batch_attention_masks,
            model_embedding_layer,
            self.tokenizer.pad_token_id,
            self.model.device
        )

        seq_len = final_inputs_embeds.size(1)
        origin_labels = final_labels  

        if seq_len > MAX_LEN:
            start_index = seq_len - MAX_LEN
            final_inputs_embeds = final_inputs_embeds[:, start_index:, :]
            final_labels = final_labels[:, start_index:] 
            final_attention_mask = final_attention_mask[:, start_index:]
        
        # model forward
        model_dtype = next(self.model.parameters()).dtype
        final_inputs_embeds = final_inputs_embeds.to(model_dtype)
        
        outputs = self.model.forward(
            inputs_embeds=final_inputs_embeds,
            attention_mask=final_attention_mask,
            labels=final_labels,
            output_hidden_states=False,
            output_attentions=False,
            use_cache=False,       
            return_dict=True       
        )

        logits = outputs.logits
        B, truncated_len, D = logits.shape   # batch, truncated_len, vocab_size
        
        if seq_len > MAX_LEN:
            padded_logits = torch.zeros(B, seq_len, D, dtype=logits.dtype, device=logits.device)
            start_index = seq_len - MAX_LEN
            padded_logits[:, start_index:, :] = logits 
            outputs.logits = padded_logits
        else:
            outputs.logits = logits
        
        if seq_len > MAX_LEN:
            padded_labels = origin_labels.clone()
            start_index = seq_len - MAX_LEN
            padded_labels[:, :start_index + 1] = -100
            outputs["labels"] = padded_labels
        else:
            outputs["labels"] = origin_labels

        self._reconstruct_state(origin_state)
        
        return outputs
    
    @torch.no_grad()
    def invoke(
        self, 
        system_batch_inputs: dict[str, list[str]], 
        user_batch_inputs: dict[str, list[str]], 
        generation_config: GenerationConfig,
        **kwargs
    ) -> list[MessageNode]:
        batch_inputs_embeds = []
        batch_input_ids = []
        batch_size = len(next(iter(system_batch_inputs.values())))
        
        batch_system_fields: list[dict[str]] = []
        batch_user_fields: list[dict[str]] = []
        for i in range(batch_size):
            system_sample = {k: v[i] for k, v in system_batch_inputs.items()}
            user_sample = {k: v[i] for k, v in user_batch_inputs.items()}
            
            task_description = user_sample.get("task_description")
            experience = self._trigger_memory(task_description)
            user_sample["memory_content"] = experience.text_memory
            # NOTE - The `role_message` field in extra_fields stores all responses of this agent in the MAS.
            # During the invoke stage, the contents of extra_fields are not used, but during SFT,
            # latent_memory needs to be reconstructed based on fields in extra_fields.
            # Therefore, during the invoke stage, the contents of extra_fields must be saved to the database
            # for constructing the data required for SFT.
            user_sample["extra_fields"] = experience.extra_fields  

            # construct the text prompt input for the LLM
            system_str, user_str = self._format_llm_input_prompt(
                system_fields=system_sample,
                user_fields=user_sample
            )

            # construct the complete prompts
            full_prompt_tokens = self.tokenizer.apply_chat_template(
                [{"role": "system", "content": system_str},
                {"role": "user", "content": user_str}],
                add_generation_prompt=True,
                enable_thinking=False,
                return_tensors="pt"
            ) 

            model_embedding_layer = self.model.get_input_embeddings()
            text_embeddings = model_embedding_layer(full_prompt_tokens.to(self.model.device))
            
            # latent embedding concatenation: directly append the extracted memory to all text inputs
            latent_emb = experience.latent_memory.to(self.model.device) if experience.latent_memory is not None else None

            if latent_emb is not None:
                if latent_emb.dim() == 2:
                    latent_emb = latent_emb.unsqueeze(0)
                
                final_inputs_embeds = torch.cat([text_embeddings, latent_emb], dim=1)
                batch_inputs_embeds.append(final_inputs_embeds.squeeze(0)) 
                batch_input_ids.append(None)
            else:
                batch_inputs_embeds.append(text_embeddings.squeeze(0))
                batch_input_ids.append(full_prompt_tokens.squeeze(0))
            
            batch_system_fields.append(system_sample)
            batch_user_fields.append(user_sample)
        
        # --- preprocess inputs (add pad tokens) ---
        # pad batch embeddings
        max_seq_len = max([emb.shape[0] for emb in batch_inputs_embeds])
        padded_inputs_embeds = []
        for emb in batch_inputs_embeds:
            padding_needed = max_seq_len - emb.shape[0]
            if padding_needed > 0:
                
                padding_tensor = torch.zeros(
                    (padding_needed, emb.shape[1]), 
                    dtype=emb.dtype, 
                    device=self.model.device
                )

                padded_emb = torch.cat([padding_tensor, emb], dim=0)
            else:
                padded_emb = emb
            padded_inputs_embeds.append(padded_emb) 
            
        final_batch_inputs_embeds = torch.stack(padded_inputs_embeds, dim=0)

        # construct attention mask
        attention_mask = torch.zeros(
            (final_batch_inputs_embeds.shape[0], final_batch_inputs_embeds.shape[1]), 
            device=self.model.device
        )
        for i, emb in enumerate(batch_inputs_embeds):

            original_length = emb.shape[0]
            padding_length = max_seq_len - original_length
            attention_mask[i, padding_length:max_seq_len] = 1
        
        # --- llm generate ---        
        output_ids = self.model.generate(
            inputs_embeds=final_batch_inputs_embeds, 
            attention_mask=attention_mask,
            generation_config=generation_config
        )

        # --- postprocess ---
        output_ids = erase_after_first_eos(output_ids, self.tokenizer.pad_token_id, self.tokenizer.eos_token_id)
        output_strs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        message_nodes = []
        for system_fields, user_fields, output_str in zip(batch_system_fields, batch_user_fields, output_strs):
            node = MessageNode(
                system_prompt_template=self.system_prompt_template,
                system_prompt_fields=system_fields,
                user_prompt_template=self.user_prompt_template,
                user_prompt_fields=user_fields,
                response=output_str,
                state=self._state
            )
            message_nodes.append(node)       

        return message_nodes
    
    @property
    def _state(self) -> dict:
        state = {}

        for field_name, value in self.__dict__.items():
            if field_name in ["model", "tokenizer", "centralized_memory"]:
                if value is None:
                    state[field_name] = None
                else:
                    state[field_name] = str(type(value).__name__)
            else:
                state[field_name] = value

        return state
    
    def _reconstruct_state(self, state: dict):
        for f in fields(self):
            if f.name not in ["model", "tokenizer", "centralized_memory"]:
                setattr(self, f.name, state.get(f.name))

    def _trigger_memory(self, task_description: str) -> Memory:
        """
        The agent activates memory to obtain text memory and latent memory, and performs post-processing:
        - If latent memory is not None: keep the latent memory consistent with the type of agent.model, and set text memory to Empty so it is no longer used.
        - If text memory is None: default it to Empty.
        """
        if self.centralized_memory is not None:
            memory = self.centralized_memory.retrieve_memory(task_description, self)
            
            # If latent memory exists, the agent does not use text memory
            if memory.latent_memory is not None:
                memory.text_memory = EMPTY_MEMORY
                memory.latent_memory = memory.latent_memory.to(self.model.dtype)
            
            # If text memory does not exist, use the EMPTY_MEMORY placeholder instead
            if memory.text_memory is None:
                memory.text_memory = EMPTY_MEMORY
        else:
            memory = Memory(text_memory=EMPTY_MEMORY, latent_memory=None)
        
        return memory
    
    def _recover_memory(self, text_memory: str, task_description: str, extra_fields: dict) -> Memory:
        # obtain the compressed latent memory based on the task description and extra_fields
        if self.centralized_memory is not None:
            assert text_memory is not None
            memory = self.centralized_memory.process_memory(text_memory, task_description, extra_fields, self)
        else:
            assert text_memory is None 
            memory = Memory(text_memory=EMPTY_MEMORY, latent_memory=None)
        
        if memory.latent_memory is not None:
            memory.latent_memory = memory.latent_memory.to(self.model.dtype)
        
        return memory

    def _safe_format(self, template: str, fields: dict) -> str:
        formatter = Formatter()
        field_names = {fname for _, fname, _, _ in formatter.parse(template) if fname}

        missing_fields = field_names - fields.keys()
        if missing_fields:
            raise KeyError(f"Missing fields for formatting: {missing_fields}")

        safe_fields = {k: v for k, v in fields.items() if k in field_names}
        return template.format(**safe_fields)
    
    def _format_llm_input_prompt(self, system_fields: dict, user_fields: dict) -> tuple[str, str]:
        """
        Safely format system and user prompts.
        Any keys in fields that do not appear in the template are ignored.
        """
        system_prompt = self._safe_format(self.system_prompt_template, system_fields)
        user_prompt = self._safe_format(self.user_prompt_template, user_fields)

        return system_prompt, user_prompt